{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FLwqmYiEXRt3",
    "outputId": "46e3c221-9780-409b-de7d-656e6b2382c6"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kagglehub'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/92/c6jxccp53xbf5dt0lfvv0_wr0000gn/T/ipykernel_24655/408812190.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Ref: https://www.kaggle.com/datasets/jeromeblanchet/yale-universitys-spider-10-nlp-dataset/data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkagglehub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdataset_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkagglehub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"jeromeblanchet/yale-universitys-spider-10-nlp-dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'kagglehub'"
     ]
    }
   ],
   "source": [
    "# Ref: https://www.kaggle.com/datasets/jeromeblanchet/yale-universitys-spider-10-nlp-dataset/data\n",
    "!pip install kaggle\n",
    "\n",
    "import kagglehub\n",
    "dataset_path = kagglehub.dataset_download(\"jeromeblanchet/yale-universitys-spider-10-nlp-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 710
    },
    "id": "vlrxsyU13ju8",
    "outputId": "ddf4c32e-94e2-45c2-e94a-a331070f9654"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "spider_path = os.path.join(dataset_path, \"spider\")\n",
    "data_path = os.path.join(spider_path, \"train_spider.json\")\n",
    "\n",
    "with open(data_path, \"r\") as f:\n",
    "  data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F0jJDO8NatUG"
   },
   "outputs": [],
   "source": [
    "# Ref: https://github.com/jszheng/py3antlr4book\n",
    "# Ref: https://www.antlr.org/download.html\n",
    "# !pip install antlr4-python3-runtime\n",
    "\n",
    "# !wget https://www.antlr.org/download/antlr-4.9.3-complete.jar -O antlr.jar\n",
    "\n",
    "# ANTLR_JAR = \"antlr.jar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7iDG2STNf1wr"
   },
   "outputs": [],
   "source": [
    "# !rm -f SimpleSQLParser.py SimpleSQLParser.tokens SimpleSQLParser.interp\n",
    "# !rm -f SimpleSQLListener.py SimpleSQLVisitor.py\n",
    "# !rm -f SimpleSQL.interp SimpleSQL.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mxLgq9T6h4tw"
   },
   "outputs": [],
   "source": [
    "# Ref: https://dzone.com/articles/building-sql-to-dataframe-converter-with-antlr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P6ePpT4_c3q9"
   },
   "outputs": [],
   "source": [
    "# !java -jar antlr.jar -Dlanguage=Python3 SimpleSQL.g4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o8e97qp_c-uf"
   },
   "outputs": [],
   "source": [
    "# Ref: https://dzone.com/articles/how-to-perform-custom-error-handling-with-antlr\n",
    "from antlr4.error.ErrorListener import ErrorListener\n",
    "\n",
    "class GuidedTokenErrorListener(ErrorListener):\n",
    "  def __init__(self):\n",
    "    self.syntax_errors = []\n",
    "\n",
    "  def syntaxError(self, recognizer, offendingSymbol, line, column, msg, e):\n",
    "    self.syntax_errors.append(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c5PVfh8ZuDMR"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "tables_data = os.path.join(os.path.join(dataset_path, \"spider\"), \"tables.json\")\n",
    "\n",
    "with open(tables_data, \"r\") as f:\n",
    "  spider_tables = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xh61dZO8PWGh"
   },
   "outputs": [],
   "source": [
    "from SimpleSQLLexer import SimpleSQLLexer\n",
    "from SimpleSQLParser import SimpleSQLParser\n",
    "from antlr4 import InputStream, CommonTokenStream\n",
    "\n",
    "class GuidedTokenGenerator:\n",
    "  def __init__(self, pii_tokens, llm, get_schema):\n",
    "    self.pii_tokens = {t.lower() for t in pii_tokens}\n",
    "    self.llm = llm\n",
    "    self.get_schema = get_schema\n",
    "\n",
    "    self.user_prompt = None\n",
    "    self.db = None\n",
    "    self.generated_tokens = []\n",
    "    self.candidate_tokens = []\n",
    "    self.is_finished = False\n",
    "\n",
    "    self.schema_text = None\n",
    "    self.schema_tables = None\n",
    "    self.schema_cols = set()\n",
    "\n",
    "    self.exhausted_paths = {}\n",
    "\n",
    "  def start(self, prompt, db=None):\n",
    "    self.user_prompt = prompt\n",
    "    self.db = db\n",
    "\n",
    "    # store DB schema\n",
    "    schema_dict = self.get_schema(self.db)\n",
    "    self.schema_tables = schema_dict[\"tables\"]\n",
    "    self.schema_text = (\"Database schema:\\n\"\n",
    "      f\"Tables: {', '.join(schema_dict['tables'])}\\n\"\n",
    "      \"Table Columns:\\n\" +\n",
    "      \"\\n\".join(f\"  - {table}: {', '.join(cols)}\"\n",
    "        for table, cols in schema_dict[\"table_columns\"].items()\n",
    "      )\n",
    "    )\n",
    "\n",
    "    # store all schema-related Tokens\n",
    "    tokens = set()\n",
    "    for cols in schema_dict[\"table_columns\"].values():\n",
    "      tokens.update(cols)\n",
    "    self.schema_cols = {t.lower() for t in tokens}\n",
    "\n",
    "    # reset all state\n",
    "    self.generated_tokens = []\n",
    "    self.candidate_tokens = []\n",
    "    self.is_finished = False\n",
    "    self.exhausted_paths = {}\n",
    "\n",
    "    return self.db\n",
    "\n",
    "  # extract tokens based on what is expected from the Simple SQL Grammar\n",
    "  def extract_from_errors(self, errors):\n",
    "    if not errors:\n",
    "      return set()\n",
    "\n",
    "    errors_match = re.search(r\"expecting (.*)\", errors[0])\n",
    "    if not errors_match:\n",
    "      return set()\n",
    "\n",
    "    text = errors_match.group(1).strip().strip(\"{}\")\n",
    "    unfiltered = re.findall(r\"'[^']+'|[A-Za-z_]+\", text)\n",
    "    return {token.strip(\"'\") for token in unfiltered}\n",
    "\n",
    "  def next_grammatical_tokens(self):\n",
    "    prefix = \" \".join(self.generated_tokens)\n",
    "    input_stream = InputStream(prefix)\n",
    "\n",
    "    lexer = SimpleSQLLexer(input_stream)\n",
    "    token_stream = CommonTokenStream(lexer)\n",
    "\n",
    "    parser = SimpleSQLParser(token_stream)\n",
    "\n",
    "    error_listener = GuidedTokenErrorListener()\n",
    "    parser.removeErrorListeners()\n",
    "    parser.addErrorListener(error_listener)\n",
    "\n",
    "    try:\n",
    "      parser.query()\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "    if error_listener.syntax_errors:\n",
    "      return self.extract_from_errors(error_listener.syntax_errors)\n",
    "    return set()\n",
    "\n",
    "  def safe_tokens(self):\n",
    "    last_token = self.generated_tokens[-1] if self.generated_tokens else \"\"\n",
    "\n",
    "    if last_token == \"FROM\":\n",
    "      return {table_name for table_name in self.schema_tables if table_name not in self.pii_tokens}\n",
    "\n",
    "    expanded_tokens = set()\n",
    "\n",
    "    # manually adding operators because parse tree is not great\n",
    "    operators = {\"AND\", \"OR\", \"=\", \"!=\", \"<\", \">\", \"<=\", \">=\", \"IN\", \"LIKE\", \"BETWEEN\"}\n",
    "    if sum(1 for t in self.generated_tokens if t in operators) < sum(1 for t in self.generated_tokens if t in {\"WHERE\", \"HAVING\"}):\n",
    "      expanded_tokens.update(operators)\n",
    "\n",
    "    if last_token in self.schema_tables:\n",
    "      expanded_tokens.update({\"WHERE\", \"GROUP\", \"LIMIT\", \"JOIN\"})\n",
    "\n",
    "    grammar_tokens = self.next_grammatical_tokens()\n",
    "    for token in grammar_tokens:\n",
    "      if token == \"IDENT\":\n",
    "        expanded_tokens.update(self.schema_cols)\n",
    "        continue\n",
    "      if token == \"FROM\":\n",
    "        expanded_tokens.update(',')\n",
    "      expanded_tokens.add(token)\n",
    "\n",
    "    safe_tokens = {token for token in expanded_tokens if token not in self.pii_tokens}\n",
    "\n",
    "    index = len(self.generated_tokens)\n",
    "    if index in self.exhausted_paths:\n",
    "      safe_tokens = {token for token in safe_tokens if token not in self.exhausted_paths[index]}\n",
    "\n",
    "    return safe_tokens\n",
    "\n",
    "  def ask_llm(self):\n",
    "    token_list = sorted(self.candidate_tokens)\n",
    "\n",
    "    prompt = (\n",
    "      \"You are selecting the NEXT SQL TOKEN for an autocomplete system.\\n\"\n",
    "      \"You MUST behave like a deterministic parser, not a natural language model.\\n\"\n",
    "      f\"User question: {self.user_prompt}\\n\"\n",
    "      f\"Current SQL prefix: \\\"{\" \".join(self.generated_tokens)}\\\"\\n\\n\"\n",
    "      f\"Tokens already selected so far: {\", \".join(self.generated_tokens)}\\n\"\n",
    "      f\"Database schema:\\n{self.schema_text}\\n\\n\"\n",
    "      f\"Valid next tokens (choose ONLY from this list):{\", \".join(token_list)}\\n\"\n",
    "      \"IMPORTANT RULES (you MUST obey all):\"\n",
    "      \"1. You MUST choose exactly ONE token from the list.\\n\"\n",
    "      \"2. NEVER select a column that is already present in the SQL prefix after SELECT.\\n\"\n",
    "      \"3. NEVER repeat the same column twice.\\n\"\n",
    "      \"4. The Current SQL Prefix + the Token you choose MUST answer the user's question.\\n\"\n",
    "      \"4. If the SQL Query is incomplete, or the user's question has not been answered, you MUST CHOOSE A TOKEN.\\n\\n\"\n",
    "\n",
    "      \"YOU MUST return exactly one token FROM THE TOKEN LIST, or <STOP> (look at STOP CONDITION below). DO NOT RETURN ANY OTHER STRINGS.\\n\"\n",
    "\n",
    "      \"STOP CONDITION:\\n\"\n",
    "      \"If the current SQL prefix already forms a complete SQL query AND it fully answers the user's question.\\n\"\n",
    "      \"(e.g., SELECT ... FROM ..., or SELECT ... FROM ... WHERE col = literal, or SELECT ... FROM ... WHERE col = literal LIMIT 100),\\n\"\n",
    "      \"then return ONLY: <STOP>.\\n\"\n",
    "      \"Only return <STOP> if the SQL ALREADY contains both: SELECT <columns>, FROM <table>\\n\"\n",
    "      \"Do NOT return <STOP> after just SELECT, or SELECT column, or SELECT column FROM.\\n\"\n",
    "    )\n",
    "\n",
    "    llm_output = self.llm(prompt).strip().replace('\"', '').replace(\"'\", \"\").strip()\n",
    "\n",
    "    print(\"LLM Choice: \", llm_output)\n",
    "\n",
    "    # LLM wants to STOP\n",
    "    if llm_output == \"<STOP>\":\n",
    "      tokens = self.generated_tokens\n",
    "      if self.is_complete(tokens):\n",
    "        self.is_finished = True\n",
    "      return None\n",
    "\n",
    "    if \"USER_DEFINED_NUMBER\" in token_list and re.fullmatch(r\"\\d+\", llm_output):\n",
    "      return llm_output\n",
    "\n",
    "    if \"USER_DEFINED_STRING\" in token_list:\n",
    "      return llm_output\n",
    "\n",
    "    # only accept valid tokens\n",
    "    if llm_output in token_list:\n",
    "      return llm_output\n",
    "    return None\n",
    "\n",
    "  # necessary for backtracking\n",
    "  def exhaust_token(self, index, token):\n",
    "    if index not in self.exhausted_paths:\n",
    "        self.exhausted_paths[index] = set()\n",
    "    if token is not None: # we have exhausted all paths that take token at sql_query[index]\n",
    "        self.exhausted_paths[index].add(token)\n",
    "\n",
    "  def forward(self):\n",
    "    prefix = \" \".join(self.generated_tokens) # what has been generated so far/confirmed\n",
    "    self.candidate_tokens = self.safe_tokens() # tokens that can come next (candidates)\n",
    "\n",
    "    if not self.candidate_tokens: # there are no more grammatically accurate tokens that can be generated\n",
    "        tokens = prefix.split()\n",
    "        if self.is_complete(tokens): # check if the SQL query can be marked as complete\n",
    "          self.is_finished = True\n",
    "          return {\"status\": \"SQL query complete\", \"LLM choice\": \"N/A\"}\n",
    "        # backtrack\n",
    "        exhausted_token = self.generated_tokens.pop()\n",
    "        self.exhaust_token(len(self.generated_tokens), exhausted_token)\n",
    "        return {\"status\": \"Backtrack\", \"LLM choice\": \"N/A\"}\n",
    "\n",
    "    llm_choice = self.ask_llm() # ask LLM to choose from candidate token list\n",
    "\n",
    "    # if choice is invalid or LLM didn't choose at all, backtrack or mark as complete\n",
    "    if llm_choice is None:\n",
    "      if self.is_finished:\n",
    "        return {\"status\": \"SQL query complete\", \"LLM choice\": llm_choice}\n",
    "      # backtrack\n",
    "      exhausted_token = self.generated_tokens.pop()\n",
    "      self.exhaust_token(len(self.generated_tokens), exhausted_token)\n",
    "      return {\"status\": \"Backtrack\", \"LLM choice\": llm_choice}\n",
    "\n",
    "    self.generated_tokens.append(llm_choice)\n",
    "    return {\"LLM choice\": llm_choice, \"SQL prefix\": list(self.generated_tokens)}\n",
    "\n",
    "  def is_complete(self, tokens):\n",
    "    if \"SELECT\" not in tokens or \"FROM\" not in tokens:\n",
    "      return False\n",
    "\n",
    "    if tokens.index(\"FROM\") == len(tokens) - 1:\n",
    "      return False\n",
    "\n",
    "    if \"WHERE\" in tokens:\n",
    "      if len(tokens) - tokens.index(\"WHERE\") < 4:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "  def finished(self):\n",
    "    return self.is_finished\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-nUAGZpaA_I"
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=\"AIzaSyCOH9C9_encZrmYjWRWzxX6Hv16ykJPjcg\")\n",
    "\n",
    "llm = genai.GenerativeModel(\"gemini-2.5-flash-lite\")\n",
    "\n",
    "def gemini_llm(prompt: str) -> str:\n",
    "  response = llm.generate_content(prompt, generation_config={\"temperature\": 0, \"max_output_tokens\": 20})\n",
    "  return response.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hIDIA8w0kMXE"
   },
   "outputs": [],
   "source": [
    "def get_schema(db_id):\n",
    "  for db in spider_tables:\n",
    "    if db[\"db_id\"] == db_id:\n",
    "      table_names = [t.lower() for t in db['table_names_original']]\n",
    "      cols = db['column_names_original']\n",
    "      table_columns = {table_name: [] for table_name in table_names}\n",
    "      for idx, col in cols:\n",
    "        if idx != -1:\n",
    "          table_columns[table_names[idx]].append(col)\n",
    "      return {\"tables\": table_names, \"table_columns\": table_columns}\n",
    "  return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EeM3RiW-Rxk0"
   },
   "outputs": [],
   "source": [
    "guided_token_generator = GuidedTokenGenerator(\n",
    "  pii_tokens={\"ssn\", \"email\"},\n",
    "  llm=gemini_llm,\n",
    "  get_schema=get_schema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-GhTl2W928IT"
   },
   "outputs": [],
   "source": [
    "def GuidedTokenGeneration(user_question, db):\n",
    "  db_name = guided_token_generator.start(user_question, db)\n",
    "  print(\"DB:\", db_name)\n",
    "\n",
    "  attempts = 0\n",
    "\n",
    "  while not guided_token_generator.finished() and attempts < 20:\n",
    "    out = guided_token_generator.forward()\n",
    "    attempts += 1\n",
    "    print(f\"{attempts}: {out}\")\n",
    "\n",
    "  return \" \".join(guided_token_generator.generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "id": "fVBq7PJMSVnf",
    "outputId": "d9a61558-94d5-4435-e2a5-25b4c69d5ca8"
   },
   "outputs": [],
   "source": [
    "# user_question = \"How many heads of the departments are older than 56?\"\n",
    "# user_question = \"List the creation year, name and budget of each department.\"\n",
    "# user_question = \"What is the average number of employees of the departments whose rank is between 10 and 15?\"\n",
    "user_question = \"What are the names of the heads who are born outside the California state?\"\n",
    "\n",
    "sql_query = GuidedTokenGeneration(user_question,'department_management')\n",
    "print(\"Final SQL Query: \", sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VvO8s1mv5Mfk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
